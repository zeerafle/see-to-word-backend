# SeeToWord Backend

This is the backend for an image captioning service. It leverages Azure Cognitive Services to analyze images and provide a caption as well as OCR results. The service is built with FastAPI and uses environment variables to store sensitive configuration details such as your Azure API key and endpoint.

## Table of Contents

- [Table of Contents](#table-of-contents)
- [Features](#features)
- [Requirements](#requirements)
- [Installation](#installation)
- [Configuration](#configuration)
- [Running the Server](#running-the-server)
- [API Endpoints](#api-endpoints)
  - [Health Check](#health-check)
  - [Image Analysis](#image-analysis)
- [Usage](#usage)

## Features

- Accepts base64-encoded image data via a POST request.
- Uses Azure Cognitive Services to analyze images.
- Returns generated image captions and OCR (text recognition) results.
- Built with FastAPI for a fast and asynchronous backend.

## Requirements

- Python 3.12 or higher
- [Azure Cognitive Services](https://azure.microsoft.com/services/cognitive-services/) account with Azure AI Vision enabled
- Environment variable configuration for sensitive keys

## Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/zeerafle/see-to-word-backend
   cd see-to-word-backend
   ```

2. **Create a virtual environment:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Linux/Mac
   # On Windows use: venv\Scripts\activate
   ```

3. **Install dependencies:**

   This project is managed using uv. First install uv, then sync the dependencies.

   ```bash
   pip install uv
   uv sync
   ```

## Configuration

Before running the application, create a .env file in the project root based on the provided .env.sample file. The .env file should contain your Azure Cognitive Services configuration:

```env
AI_SERVICES_KEY=your_azure_api_key_here
AI_SERVICES_ENDPOINT=your_azure_endpoint_here
AI_SERVICES_REGION=your_azure_region_here
```

**Note:** Do not commit your actual API keys to version control.

## Running the Server

To run the FastAPI server in development mode with auto-reload, use [uvicorn](https://www.uvicorn.org/):

```bash
uvicorn main:app --reload
```

Your API will be available at: [http://localhost:8000](http://localhost:8000)

## API Endpoints

### Health Check

- **GET /**
  Returns a simple greeting message.

  **Example Response:**

  ```json
  {
    "Hello": "World"
  }
  ```

### Image Analysis

- **POST /image-analysis**

  Accepts a JSON payload with a base64 encoded image and returns the analyzed results (caption + OCR data).

  **Request Body:**

  ```json
  {
    "base64_image": "your_base64_encoded_image_string_here"
  }
  ```

  **Response Example:**

  ```json
  {
    "caption": {
      "text": "A descriptive caption generated by Azure",
      "confidence": 0.9876
    },
    "read": [
      {
        "text": "Detected text line",
        "bounding_polygon": [ ... ],
        "words": [
          {
            "text": "word",
            "bounding_polygon": [ ... ],
            "confidence": 0.9543
          }
        ]
      }
    ]
  }
  ```

## Usage

1. **Prepare your base64 encoded image:**

   Convert an image to a base64 string. There are many online tools or use this Python snippet:

   ```python
   import base64

   with open("path/to/image.jpg", "rb") as image_file:
       base64_string = base64.b64encode(image_file.read()).decode("utf-8")
       print(base64_string)
   ```

2. **Send a request:**

   Use Postman or any HTTP client to send a POST request to `/image-analysis` with the JSON payload containing your base64 image string.
